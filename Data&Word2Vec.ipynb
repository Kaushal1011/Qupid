{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expected-damages",
   "metadata": {},
   "source": [
    "# Notebook about the research done on retrieving data for word2vec embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-print",
   "metadata": {},
   "source": [
    "## Trying to find related words using twitter tweet search api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-belarus",
   "metadata": {},
   "source": [
    "## Define all base terms for which we want to search tweets for that may come in member profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms=[\"Programmer\",\"Coding\",\"Javascript\",\"Cloud Computing\",\"Manager\",\"Finance\",\"Design\",\"Art\",\"Dribbble\",\"Github\",\"Python\",\"Data\",\"Science\",\"Commerce\",\"Machine Learning\",\\\n",
    "#       \"Engineer\",\"IBM\",\"Chartered Accountant\",\"Figma\",\"Adobe\",\"Music\",\"Audio\",\"Language\",\"Vocabulary\",\"Designer\",\"Coder\",\"Mathematician\",\"Creative\",\"Logic\",\"Logical\"]\n",
    "# terms specially listed for teams app \n",
    "terms=[\"Programmer\",\"Coding\",\"Deep Learning\",\"Cloud Computing\",\"Javascript\",\"Backend\",\"Frontend\",\"Data Science\",\"Dribbble\",\"Github\",\"Machine Learning\",\\\n",
    "       \"Engineer\",\"Figma\",\"Sketch\",\"Blender\",\"Mathematician\",\"Audio Programming\",\"ReactJS\",\"VueJS\",\"CSS\",\"Design Engineer\",\"Operating System\",\\\n",
    "       \"Linux\",\"Scripting\",\"Shell Script\",\"Leader\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smart-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Programmer',\n",
       " 'Coding',\n",
       " 'Deep Learning',\n",
       " 'Cloud Computing',\n",
       " 'Javascript',\n",
       " 'Backend',\n",
       " 'Frontend',\n",
       " 'Data Science',\n",
       " 'Dribbble',\n",
       " 'Github',\n",
       " 'Machine Learning',\n",
       " 'Engineer',\n",
       " 'Figma',\n",
       " 'Sketch',\n",
       " 'Blender',\n",
       " 'Mathematician',\n",
       " 'Audio Programming',\n",
       " 'ReactJS',\n",
       " 'VueJS',\n",
       " 'CSS',\n",
       " 'Design Engineer',\n",
       " 'Operating System',\n",
       " 'Linux',\n",
       " 'Scripting',\n",
       " 'Shell Script',\n",
       " 'Leader']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-wallpaper",
   "metadata": {},
   "source": [
    "### Twitter API sample code\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "\n",
    "\n",
    "def auth():\n",
    "    return os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "def create_url():\n",
    "    query = \"from:twitterdev -is:retweet\"\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    tweet_fields = \"tweet.fields=author_id\"\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}\".format(\n",
    "        query, tweet_fields\n",
    "    )\n",
    "    return url\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    bearer_token = auth()\n",
    "    url = create_url()\n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def create_url(query):\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "#     tweet_fields = \"tweet.fields=author_id\"\n",
    "    #max_results=100\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=10\".format(\n",
    "        query\n",
    "    )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "active-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return \"AAAAAAAAAAAAAAAAAAAAAC6vMwEAAAAA5uumFGxBM1e4Kfyfr8E5TJseEZw%3DHV6mjkDp1QrpVm1bpi2yZrvAEkpqhxCK91fGkjl5gevPwtLScM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-nursing",
   "metadata": {},
   "source": [
    "```python\n",
    "# fetch tweet data initial\n",
    "for i in terms:\n",
    "    bearer_token = auth()\n",
    "    url = create_url(i)\n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "#     print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    with open(\"./word_dataset/\"+i+'_data.json', 'a') as f:\n",
    "        json.dump(json_response[\"data\"], f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-better",
   "metadata": {},
   "source": [
    "```python\n",
    "# fetch more tweet data\n",
    "def create_url(query,next_token):\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "#     tweet_fields = \"tweet.fields=author_id\"\n",
    "    #max_results=100\n",
    "    if next_token:\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=100&next_token={}\".format(\n",
    "            query,next_token\n",
    "        )\n",
    "    else:\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=100\".format(\n",
    "            query\n",
    "        )\n",
    "    return url\n",
    "\n",
    "next_token=[i for i in terms]\n",
    "iters=5\n",
    "for j in range(iters):\n",
    "    k=0\n",
    "    for i in terms:\n",
    "        bearer_token = auth()\n",
    "        if(j!=0):\n",
    "            url = create_url(i,next_token[k])\n",
    "        else:\n",
    "            url = create_url(i,None)\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(url, headers)\n",
    "        if(json_response[\"meta\"][\"next_token\"]):\n",
    "            next_token[k]=json_response[\"meta\"][\"next_token\"]\n",
    "        k+=1\n",
    "        with open(\"./word_dataset/\"+i+\"_data.json\", \"r+\") as file:\n",
    "            data = json.load(file)\n",
    "            data.extend(json_response[\"data\"])\n",
    "            file.seek(0)\n",
    "            json.dump(data, file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-nothing",
   "metadata": {},
   "source": [
    "## Start cleaning data then extract keywords using rake or spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-colombia",
   "metadata": {},
   "source": [
    "```python\n",
    "import re\n",
    "for i in terms:\n",
    "    with open(\"./word_dataset/\"+i+\"_data.json\", \"r+\") as file:\n",
    "            data = json.load(file)\n",
    "            arr=[]\n",
    "            for k in data:\n",
    "                # add clean up line here and saved cleaned data somewhere\n",
    "                final_cleaned = str(re.sub(r\"[^a-zA-Z0-9]+\", ' ', k[\"text\"]))\n",
    "                arr.append(final_cleaned)\n",
    "            file2=open(\"./word_dataset/\"+i+\"_cleaned.json\",\"a+\")\n",
    "            json.dump(arr,file2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "editorial-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(re.sub(r\"[^a-zA-Z0-9]+\", ' ', k[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-bernard",
   "metadata": {},
   "source": [
    "## Start removing stop words and useless words from sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interested-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge rake_nltk -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "choice-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'couldn', \"doesn't\", 'with', 'being', 'not', 'nor', 'most', 'yourselves', 'too', 'off', 'whom', 'at', 'some', 'been', 'because', 'were', \"weren't\", 'you', \"should've\", 'if', 'this', 'between', 'him', 'ourselves', 'now', \"didn't\", 'her', 'o', 'yourself', \"aren't\", 'we', 'to', 've', 'from', 'than', 'doesn', 'has', 'she', \"shouldn't\", 'won', 'is', 'y', 'himself', 'own', 'why', 'myself', 'further', 'did', \"won't\", \"hasn't\", 'there', 'shan', 'be', 'isn', 'theirs', 'doing', 'themselves', 'll', 'that', 'having', 'or', 'should', 'once', 'against', 'mustn', 'what', 'have', 'didn', \"mightn't\", \"isn't\", 'a', 'any', 'aren', 'm', 'do', \"she's\", 'ma', 'few', 'below', 'haven', 'of', 'shouldn', 'down', 'for', 'mightn', 'will', 'their', 't', 'when', \"wouldn't\", 'only', 'before', 'don', 'my', 'itself', 'by', 'was', \"you've\", 'd', 'ain', 'which', \"wasn't\", 'same', 'above', 'an', 'wouldn', 'again', 'yours', 'hadn', 'am', \"hadn't\", 'me', 'but', 'then', 'no', \"don't\", \"shan't\", 'does', 'such', 'who', 'hers', 'these', \"couldn't\", 'i', 'our', 'here', \"you'll\", 'the', 'wasn', 'how', 'while', 's', 'into', \"mustn't\", \"it's\", 'are', 'very', 'each', 'as', 'hasn', 'out', 'over', 'about', 'up', 'ours', 'can', 'on', \"needn't\", 're', 'under', \"you'd\", 'in', 'other', \"haven't\", 'and', 'until', 'its', \"that'll\", 'more', 'your', 'his', 'he', 'those', 'had', 'through', 'after', 'just', 'needn', 'herself', 'so', 'it', 'they', 'them', 'all', 'weren', \"you're\", 'both', 'during', 'where'}\n"
     ]
    }
   ],
   "source": [
    "# !python -c \"import nltk; nltk.download('stopwords')\"\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-lincoln",
   "metadata": {},
   "source": [
    "## Removed stop words from tweets lets build a pandas dataframe with text\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in terms:\n",
    "    with open(\"./cleaned_dataset/\"+i+\"_cleaned.json\", \"r+\") as file:\n",
    "        data = json.load(file)\n",
    "        arr=[]\n",
    "        for k in data:\n",
    "            # add clean up line here and saved cleaned data somewhere\n",
    "            final_cleaned = ' '.join([word for word in k.split() if word not in stop_words])\n",
    "            arr.append(final_cleaned)\n",
    "            file2=open(\"./cleaned_dataset/\"+i+\"_stopworded.json\",\"w+\")\n",
    "            json.dump(arr,file2)\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "former-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bizarre-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pending-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_db=[]\n",
    "for i in terms:\n",
    "     with open(\"./cleaned_dataset/\"+i+\"_stopworded.json\", \"r+\") as file:\n",
    "        data = json.load(file)\n",
    "        for k in data:\n",
    "            text_db.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "remarkable-clinton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10455"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "changing-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_df[\"text\"]=text_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "western-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gekido SykooSam I must born programmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT PR0GRAMMERHUM0R Every programmer like https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT carrotpharma Join welcoming Pharma Consulta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT motherofnodejs Never programming coding pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT motherofnodejs Never programming coding pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>RT toryboypierce Former SNP leader Alec Salmon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10451</th>\n",
       "      <td>RT Fereeha PPP leader TalpurTaimur said snake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10452</th>\n",
       "      <td>RT nowthisnews HAPPENING NOW SCHUMER SPEAKS TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10453</th>\n",
       "      <td>RT waller73 Just year leader done LiverpoolMayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10454</th>\n",
       "      <td>RT DeputyRustArt So interesting lore stuff GG ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10455 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0                 gekido SykooSam I must born programmer\n",
       "1      RT PR0GRAMMERHUM0R Every programmer like https...\n",
       "2      RT carrotpharma Join welcoming Pharma Consulta...\n",
       "3      RT motherofnodejs Never programming coding pro...\n",
       "4      RT motherofnodejs Never programming coding pro...\n",
       "...                                                  ...\n",
       "10450  RT toryboypierce Former SNP leader Alec Salmon...\n",
       "10451  RT Fereeha PPP leader TalpurTaimur said snake ...\n",
       "10452  RT nowthisnews HAPPENING NOW SCHUMER SPEAKS TO...\n",
       "10453   RT waller73 Just year leader done LiverpoolMayor\n",
       "10454  RT DeputyRustArt So interesting lore stuff GG ...\n",
       "\n",
       "[10455 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "hazardous-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_df.to_csv(\"tweet_text_db.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "criminal-sister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/kaypee/miniconda3/envs/ds\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto-2.49.0                |           py38_0         1.6 MB  anaconda\n",
      "    boto3-1.17.12              |     pyhd3eb1b0_0          70 KB\n",
      "    botocore-1.20.12           |     pyhd3eb1b0_1         3.5 MB\n",
      "    ca-certificates-2020.10.14 |                0         128 KB  anaconda\n",
      "    certifi-2020.6.20          |           py38_0         160 KB  anaconda\n",
      "    gensim-3.8.3               |   py38h2531618_2        18.5 MB\n",
      "    google-api-core-1.22.2     |           py38_0         101 KB  anaconda\n",
      "    google-cloud-core-1.4.3    |             py_0          29 KB  anaconda\n",
      "    google-cloud-storage-1.32.0|             py_0          64 KB  anaconda\n",
      "    google-crc32c-1.0.0        |   py38h7b6447c_0          26 KB  anaconda\n",
      "    google-resumable-media-1.1.0|             py_1          42 KB  anaconda\n",
      "    googleapis-common-protos-1.52.0|           py38_0          75 KB  anaconda\n",
      "    jmespath-0.10.0            |             py_0          22 KB  anaconda\n",
      "    libcrc32c-1.1.1            |       he6710b0_2          21 KB  anaconda\n",
      "    s3transfer-0.3.3           |           py38_0          96 KB  anaconda\n",
      "    smart_open-3.0.0           |             py_0          78 KB  anaconda\n",
      "    yarl-1.5.1                 |   py38h7b6447c_0         144 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        24.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boto               anaconda/linux-64::boto-2.49.0-py38_0\n",
      "  boto3              pkgs/main/noarch::boto3-1.17.12-pyhd3eb1b0_0\n",
      "  botocore           pkgs/main/noarch::botocore-1.20.12-pyhd3eb1b0_1\n",
      "  gensim             pkgs/main/linux-64::gensim-3.8.3-py38h2531618_2\n",
      "  google-api-core    anaconda/linux-64::google-api-core-1.22.2-py38_0\n",
      "  google-cloud-core  anaconda/noarch::google-cloud-core-1.4.3-py_0\n",
      "  google-cloud-stor~ anaconda/noarch::google-cloud-storage-1.32.0-py_0\n",
      "  google-crc32c      anaconda/linux-64::google-crc32c-1.0.0-py38h7b6447c_0\n",
      "  google-resumable-~ anaconda/noarch::google-resumable-media-1.1.0-py_1\n",
      "  googleapis-common~ anaconda/linux-64::googleapis-common-protos-1.52.0-py38_0\n",
      "  jmespath           anaconda/noarch::jmespath-0.10.0-py_0\n",
      "  libcrc32c          anaconda/linux-64::libcrc32c-1.1.1-he6710b0_2\n",
      "  s3transfer         anaconda/linux-64::s3transfer-0.3.3-py38_0\n",
      "  smart_open         anaconda/noarch::smart_open-3.0.0-py_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> anaconda::ca-certificates-2020.10.14-0\n",
      "  certifi            conda-forge::certifi-2020.12.5-py38h5~ --> anaconda::certifi-2020.6.20-py38_0\n",
      "  yarl                 pkgs/main::yarl-1.6.3-py38h27cfd23_0 --> anaconda::yarl-1.5.1-py38h7b6447c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "googleapis-common-pr | 75 KB     | ##################################### | 100% \n",
      "google-cloud-core-1. | 29 KB     | ##################################### | 100% \n",
      "certifi-2020.6.20    | 160 KB    | ##################################### | 100% \n",
      "yarl-1.5.1           | 144 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 128 KB    | ##################################### | 100% \n",
      "google-crc32c-1.0.0  | 26 KB     | ##################################### | 100% \n",
      "boto3-1.17.12        | 70 KB     | ##################################### | 100% \n",
      "smart_open-3.0.0     | 78 KB     | ##################################### | 100% \n",
      "botocore-1.20.12     | 3.5 MB    | ##################################### | 100% \n",
      "google-api-core-1.22 | 101 KB    | ##################################### | 100% \n",
      "gensim-3.8.3         | 18.5 MB   | ##################################### | 100% \n",
      "boto-2.49.0          | 1.6 MB    | ##################################### | 100% \n",
      "jmespath-0.10.0      | 22 KB     | ##################################### | 100% \n",
      "google-resumable-med | 42 KB     | ##################################### | 100% \n",
      "libcrc32c-1.1.1      | 21 KB     | ##################################### | 100% \n",
      "google-cloud-storage | 64 KB     | ##################################### | 100% \n",
      "s3transfer-0.3.3     | 96 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c anaconda gensim -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "informal-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "textile-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_df['tokenized'] = all_text_df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "different-alliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gekido SykooSam I must born programmer</td>\n",
       "      <td>[gekido, SykooSam, I, must, born, programmer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT PR0GRAMMERHUM0R Every programmer like https...</td>\n",
       "      <td>[RT, PR0GRAMMERHUM0R, Every, programmer, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT carrotpharma Join welcoming Pharma Consulta...</td>\n",
       "      <td>[RT, carrotpharma, Join, welcoming, Pharma, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT motherofnodejs Never programming coding pro...</td>\n",
       "      <td>[RT, motherofnodejs, Never, programming, codin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT motherofnodejs Never programming coding pro...</td>\n",
       "      <td>[RT, motherofnodejs, Never, programming, codin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>RT toryboypierce Former SNP leader Alec Salmon...</td>\n",
       "      <td>[RT, toryboypierce, Former, SNP, leader, Alec,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10451</th>\n",
       "      <td>RT Fereeha PPP leader TalpurTaimur said snake ...</td>\n",
       "      <td>[RT, Fereeha, PPP, leader, TalpurTaimur, said,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10452</th>\n",
       "      <td>RT nowthisnews HAPPENING NOW SCHUMER SPEAKS TO...</td>\n",
       "      <td>[RT, nowthisnews, HAPPENING, NOW, SCHUMER, SPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10453</th>\n",
       "      <td>RT waller73 Just year leader done LiverpoolMayor</td>\n",
       "      <td>[RT, waller73, Just, year, leader, done, Liver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10454</th>\n",
       "      <td>RT DeputyRustArt So interesting lore stuff GG ...</td>\n",
       "      <td>[RT, DeputyRustArt, So, interesting, lore, stu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0                 gekido SykooSam I must born programmer   \n",
       "1      RT PR0GRAMMERHUM0R Every programmer like https...   \n",
       "2      RT carrotpharma Join welcoming Pharma Consulta...   \n",
       "3      RT motherofnodejs Never programming coding pro...   \n",
       "4      RT motherofnodejs Never programming coding pro...   \n",
       "...                                                  ...   \n",
       "10450  RT toryboypierce Former SNP leader Alec Salmon...   \n",
       "10451  RT Fereeha PPP leader TalpurTaimur said snake ...   \n",
       "10452  RT nowthisnews HAPPENING NOW SCHUMER SPEAKS TO...   \n",
       "10453   RT waller73 Just year leader done LiverpoolMayor   \n",
       "10454  RT DeputyRustArt So interesting lore stuff GG ...   \n",
       "\n",
       "                                               tokenized  \n",
       "0          [gekido, SykooSam, I, must, born, programmer]  \n",
       "1      [RT, PR0GRAMMERHUM0R, Every, programmer, like,...  \n",
       "2      [RT, carrotpharma, Join, welcoming, Pharma, Co...  \n",
       "3      [RT, motherofnodejs, Never, programming, codin...  \n",
       "4      [RT, motherofnodejs, Never, programming, codin...  \n",
       "...                                                  ...  \n",
       "10450  [RT, toryboypierce, Former, SNP, leader, Alec,...  \n",
       "10451  [RT, Fereeha, PPP, leader, TalpurTaimur, said,...  \n",
       "10452  [RT, nowthisnews, HAPPENING, NOW, SCHUMER, SPE...  \n",
       "10453  [RT, waller73, Just, year, leader, done, Liver...  \n",
       "10454  [RT, DeputyRustArt, So, interesting, lore, stu...  \n",
       "\n",
       "[10455 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-alignment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
